# TEMPLATE GITLAB-CI FOR CCAO MODELS

# This CI script creates the R environment to run the CCAO's automated valuation
# models. It uses the same basic pipeline as the run.R script included with
# every model, but runs each script as a separate stage on GitLab's CI runners.

# This setup uses dependency caching to speed up build times, see below for
# more details

# Base R image to build the model with. Should be version locked to the highest
# version most CCAO employees are using locally
image: rocker/r-ver:4.1.2

# Defining static variables used throughout the CI pipeline
variables:
  DOCKER_DRIVER: "overlay2" # Docker FS driver, don't change this
  GIT_SUBMODULE_STRATEGY: "recursive" # Set how GitLab handles submodules
  # If a package has a linux dependency that isn't already listed, add it here
  APT_DEPS: "libcurl4-openssl-dev libssl-dev libxml2-dev libgit2-dev git libudunits2-dev python3-dev python3-pip"
  # Specify vars to enable caching and dependency management via renv and pip
  RENV_CONFIG_REPOS_OVERRIDE: "https://cloud.r-project.org/"
  RENV_PATHS_CACHE: ${CI_PROJECT_DIR}/cache
  RENV_PATHS_LIBRARY: ${CI_PROJECT_DIR}/renv/library
  PIP_CACHE_DIR: ${CI_PROJECT_DIR}/cache 

# Cache settings. R libraries are installed into the renv cache based
# on the libraries listed in the renv.lock file. These libraries are then
# copied between each build as a .zip file. This mitigates the need to reinstall
# libraries for every build (which takes a long time). The cache will be re-used
# until the renv.lock file changes

# The input data is also versioned using DVC. If the .dvc file of any input data
# changes, then the cache will be busted and new data will be pulled from S3
cache:
  - key:
      files:
        - renv.lock
    paths:
      - .apt
      - ${RENV_PATHS_CACHE}
      - ${RENV_PATHS_LIBRARY}
  - key:
      files:
        - ${CI_PROJECT_DIR}/input/assessment_data.parquet.dvc
        - ${CI_PROJECT_DIR}/input/training_data.parquet.dvc
    paths:
      - ${CI_PROJECT_DIR}/input

# Run all of these commands before starting any jobs
before_script:
  # These are commands for caching installed APT_DEPS, they slightly speed up
  # overall build times but aren't strictly necessary
  - rm -f /etc/apt/apt.conf.d/docker-clean
  - mkdir -p .apt && mkdir -p /var/cache/apt/archives && mount --bind .apt /var/cache/apt/archives/

  # Install apt dependencies listed in APT_DEPS variable
  - apt-get update && apt-get install --no-install-recommends -y ${APT_DEPS}
  
  # pip install DVC to fetch data from S3 using dvc pull
  - pip install aiobotocore[boto3] boto3 dvc[s3] 
  
  # Install R dependencies listed in renv.lock using renv
  - Rscript -e 'renv::restore()'

# Each of the stages listed below corresponds to a script in the pipeline/
# directory. The purpose of each stage is described in run.R. Artifacts are used
# to pass objects between stages. They expire quickly because they're typically
# uploaded to S3
.job_template: &run_stage
  artifacts:
    paths:
      - output/
    expire_in: 1 day

# Note that portions of the assess and interpret stages are skipped because
# they take too long on shared GitLab runners
stages:
  - train
  - evaluate
  - finalize

train:
  <<: *run_stage
  stage: train
  only:
    - master
  script:
    - dvc pull
    - Rscript pipeline/01-train.R

evaluate:
  <<: *run_stage
  stage: evaluate
  only:
    - master
  script:
    - Rscript pipeline/03-evaluate.R

finalize:
  <<: *run_stage
  stage: finalize
  only:
    - master
  script:
    - Rscript pipeline/05-finalize.R
