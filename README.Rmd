---
title: "Table of Contents"
output: 
  github_document:
    toc: true
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

## Model Overview

The duty of the Cook County Assessor's Office is to value property in a fair, accurate, and transparent way. The Assessor is committed to transparency throughout the assessment process, and as such, this repository contains:

* [Nearly all code used to determine Cook County residential property values](./)
* [Rationale for different modeling, feature, and code decisions that affect assessed property values](#choices-made)
* [An outline of ongoing data quality issues that affect assessed property values](#ongoing-issues)
* [Instructions to replicate our model and results using open data](#installation)

The repository itself contains code for the Computer Assisted Mass Appraisal (CAMA) system used to generate initial assessed values for all single-family and multi-family residential properties in Cook County. This system is effectively an advanced statistical/machine learning model (hereafter referred to as "the model") which uses previous sales to generate predicted values (assessments) for unsold properties. 

Note that data extraction/preparation, feature engineering, and data validation for this model are handled in a [separate repository](https://gitlab.com/ccao-data-science---modeling/processes/etl_res_data). Values for [residential condominiums](https://gitlab.com/ccao-data-science---modeling/models/ccao_condo_avm) and [commercial apartments](https://gitlab.com/ccao-data-science---modeling/models/commercial-apartments-automated-valuation-model) are determined by separate models.

### How It Works

The goal of the model is to answer the question "What would the sale price of every home be if it had sold last year?" To answer this question, we use a two-step process:

1. **Modeling**: First, we use the code in this repository to train an advanced machine learning model. The model predicts the sale price (fair market value) of unsold properties using the known sale price of similar and nearby properties. Training the model involves iteratively updating a mathematical function to recognize patterns in the data. The output of this step is a model object which can be used to predict any property's sale price given a [set of characteristics (such as location, number of bedrooms, etc.)](#features-used).

2. **Valuation**: Second, we use the model created in step one to predict values for all residential properties in Cook County. We then train a secondary, much simpler model (which we call the post-modeling adjustment model) to correct for any systemic bias introduced by the first model. Finally, we combine the first and second model to produce initial assessed property values - the ones printed on Residential Reassessment Notices that are mailed when properties are reassessed. However, note that values produced by the model and mailed values may not be identical, as there are additional rounds of automated and human review between modeling and mailing.

The full residential modeling pipeline, from raw data to final values, looks something like:

``` mermaid
graph LR
    as400[("AS-400/<br>Mainframe")]
    ext_data["External data<br>(Census data,<br>geospatial)"]
    etl_pinlocations("Ext. data joined<br>to property data")
    sql[("SQL<br>database")]
    etl_res_data("Data extracted<br>and cleaned")
    data_train["Training<br>(sales) data"]
    data_ass["All property<br>data"]

    model_step1("Model training<br>(Step 1 above)")
    model_step2("Valuation<br>(Step 2 above)")
    model{"Trained<br>Model"}
    final_vals["Final predicted/<br>assessed values"]

    click etl_res_data "https://gitlab.com/ccao-data-science---modeling/processes/etl_res_data"
    click etl_pinlocations "https://gitlab.com/ccao-data-science---modeling/processes/etl_pinlocations"

    classDef Link fill:#90a9e8;
    class etl_res_data Link;
    class etl_pinlocations Link;

    as400 -->|"Data mirrored from<br>County mainframe"| sql
    ext_data --> etl_pinlocations
    etl_pinlocations --> sql
    sql --> etl_res_data
    etl_res_data --> data_train
    etl_res_data --> data_ass
    data_train --> model_step1
    model_step1 --> model
    model --> model_step2
    data_ass --> model_step2
    model_step2 --> final_vals
    final_vals -->|"Values uploaded after<br>hand review and correction"| as400
```

### Features Used

The residential model uses a variety of individual and aggregate features to determine a property's assessed value. We've tested a long list of possible features over time, including [walk score](https://gitlab.com/ccao-data-science---modeling/models/ccao_res_avm/-/blob/9407d1fae1986c5ce1f5434aa91d3f8cf06c8ea1/output/test_new_variables/county_walkscore.html), [crime rate](https://gitlab.com/ccao-data-science---modeling/models/ccao_res_avm/-/blob/9407d1fae1986c5ce1f5434aa91d3f8cf06c8ea1/output/test_new_variables/chicago_crimerate.html), [school districts](https://gitlab.com/ccao-data-science---modeling/models/ccao_res_avm/-/blob/9407d1fae1986c5ce1f5434aa91d3f8cf06c8ea1/output/test_new_variables/county_school_boundaries_mean_encoded.html), and many others. The features in the table below are the ones that made the cut. They're the right combination of easy to understand and impute, powerfully predictive, and well-behaved. Most of them are in use in the model as of `r Sys.Date()`.

```{r features_used, message=FALSE, echo=FALSE}
library(dplyr)
library(tidyr)
ccao::vars_dict %>%
  filter(var_is_predictor & var_name_standard != "meta_sale_price") %>%
  group_by(var_name_pretty) %>%
  mutate(row = paste0("X", row_number())) %>%
  distinct(
    `Feature Name` = var_name_pretty,
    Category = var_type,
    Type = var_data_type,
    var_value, row
  ) %>%
  mutate(Category = recode(
    Category,
    char = "Characteristic",
    econ = "Economic",
    geo = "Geospatial",
    ind = "Indicator",
    time = "Time",
    meta = "Meta"
  )) %>%
  pivot_wider(
    `Feature Name`:`Type`,
    names_from = row,
    values_from = var_value
  ) %>%
  unite("Possible Values", starts_with("X"), sep = ", ", na.rm = TRUE) %>%
  arrange(Category) %>%
  knitr::kable(format = "markdown")
```

### Sales Used



### Choices Made

- Model type
- Features to include/exclude
- How to trim the sales sample
- post-modeling adjustments

### Ongoing Issues

- Data integrity (wrong characteristics, bad sales, lack of chars)
- Low-value properties
- Multi-codes
- Multi-family
- Land valuation


### Major Changes From V1

- Whole county
- lightgbm
- tidymodels
- split codebase
- dependency management

### FAQs




## Technical Details

Modeling is implemented using the [Tidymodels](https://www.tidymodels.org/) framework for R. 

LightGBM + glmnet (elasticnet)

lgbm integrated with treesnip + custom code

preprocessing in recipes

Tuned according to lgbm docs

Minimized on RMSE

Other models tried






## Replication/Usage

### Installation

The code in this repository is written primarily in [R](https://www.r-project.org/about.html). Please install the [latest version of R](https://cloud.r-project.org/) and [RStudio](https://rstudio.com/products/rstudio/download/) before proceeding with the steps below. If you're on Windows, you'll also need to install [rtools40](https://cran.r-project.org/bin/windows/Rtools/) in order to build packages.

1. Clone this repository using git, or simply download using the button at the top of the page
2. Set your working directory to the local folder containing this repository's files, either using R's `setwd()` command or using RStudio's [projects](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)
3. Install `renv`, R's package manager, by running `install.packages("renv")`
4. Install all R package dependencies using `renv` by running `renv::restore()`
5. Place modeling and assessment data parquet files in the `input/` directory within the repo
6. Run the `model.R` script. This trains the model, evaluates a test set, and generates a report on the model's peformance
7. Run the `valuation.R` script. This creates a secondary adjustment model and generates predicted values for all properties we need to assess

### Usage/Files

### Troubleshooting

#### Installation

The dependencies for this repository are numerous and not all of them may install correctly. Here are some common install issues (as seen in the R console) as well as their respective resolutions:

- Error: `Failed to retrieve package 'treesnip'`
<br>Solution: Manually install treesnip [from GitHub](https://github.com/curso-r/treesnip), following the instructions listed

- Error: `WARNING: Rtools is required to build R packages, but is not currently installed`
<br>Solution: Install the latest version of Rtools [from CRAN](https://cran.r-project.org/bin/windows/Rtools/)
